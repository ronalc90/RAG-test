# src/db_iris.py
from __future__ import annotations
import json, pyodbc
from typing import List, Dict, Any, Tuple, Optional
import numpy as np
from .config import IRIS_DSN, IRIS_USER, IRIS_PASSWORD

def get_conn():
    # Requiere DSN ODBC (64-bit) creado y probado en Windows
    return pyodbc.connect(f"DSN={IRIS_DSN};UID={IRIS_USER};PWD={IRIS_PASSWORD}")

# Usamos nombres de tablas "secop_*" (pueden ser distintas a SQLite; lo importante es que las funciones
# expongan la misma INTERFAZ que espera api.py)
DDL_DOCS = """
CREATE TABLE IF NOT EXISTS secop_documents (
    doc_id INTEGER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
    titulo VARCHAR(500) NOT NULL,
    entidad VARCHAR(200),
    source_path VARCHAR(500),
    metadata LONGVARCHAR
)
"""

DDL_CHUNKS = """
CREATE TABLE IF NOT EXISTS secop_chunks (
    chunk_id INTEGER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
    doc_id INTEGER NOT NULL,
    ord INTEGER NOT NULL,
    text LONGVARCHAR NOT NULL,
    embedding_json LONGVARCHAR NOT NULL
)
"""

def init_db():
    with get_conn() as conn:
        cur = conn.cursor()
        # Algunos drivers/vers. no soportan IF NOT EXISTS en índices → try/except
        cur.execute(DDL_DOCS)
        cur.execute(DDL_CHUNKS)
        try:
            cur.execute("CREATE INDEX IF NOT EXISTS idx_chunks_docid ON secop_chunks(doc_id)")
        except Exception:
            # Fallback si IF NOT EXISTS no está soportado:
            try:
                cur.execute("CREATE INDEX idx_chunks_docid ON secop_chunks(doc_id)")
            except Exception:
                pass
        conn.commit()

def _last_identity(cur) -> Optional[int]:
    """Obtiene el último identity de la sesión (si IRIS/driver lo soporta)."""
    try:
        cur.execute("SELECT LAST_IDENTITY()")
        val = cur.fetchone()
        if val and val[0] is not None:
            return int(val[0])
    except Exception:
        pass
    return None

def insert_document(titulo: str, entidad: str|None=None, source_path: str|None=None,
                    metadata: Dict[str, Any]|None=None) -> int:
    meta = json.dumps(metadata or {}, ensure_ascii=False)
    with get_conn() as conn:
        cur = conn.cursor()
        cur.execute("""
            INSERT INTO secop_documents (titulo, entidad, source_path, metadata)
            VALUES (?, ?, ?, ?)
        """, (titulo, entidad, source_path, meta))
        new_id = _last_identity(cur)
        if new_id is None:
            # Fallback (no perfecto en alta concurrencia, pero suficiente para entornos no críticos)
            cur.execute("SELECT MAX(doc_id) FROM secop_documents")
            new_id = int(cur.fetchone()[0])
        conn.commit()
        return int(new_id)

def list_documents(limit: int = 50) -> List[Dict[str, Any]]:
    with get_conn() as conn:
        cur = conn.cursor()
        cur.execute("""
            SELECT doc_id, titulo, entidad, source_path, metadata
            FROM secop_documents
            ORDER BY doc_id DESC
            FETCH FIRST ? ROWS ONLY
        """, (limit,))
        rows = cur.fetchall()
    out: List[Dict[str, Any]] = []
    for r in rows:
        item = {
            "doc_id": int(r[0]),
            "titulo": r[1],
            "entidad": r[2],
            "archivo": r[3],     # mapeamos source_path -> "archivo" para que api.py no se rompa si lo usa
            "metadata": {}
        }
        try:
            item["metadata"] = json.loads(r[4] or "{}")
        except Exception:
            item["metadata"] = r[4]  # dejar crudo si no es JSON válido
        out.append(item)
    return out

def insert_chunks(doc_id: int, chunks: List[str], embeddings) -> int:
    """Inserta N chunks. embeddings puede ser np.ndarray o lista de listas."""
    try:
        embs_list = embeddings.tolist() if hasattr(embeddings, "tolist") else embeddings
    except Exception:
        embs_list = embeddings

    rows = []
    for i, (txt, emb) in enumerate(zip(chunks, embs_list)):
        rows.append((doc_id, i, txt, json.dumps(emb, ensure_ascii=False, separators=(",", ":"))))
    with get_conn() as conn:
        cur = conn.cursor()
        try:
            cur.fast_executemany = True
        except Exception:
            pass
        cur.executemany("""
            INSERT INTO secop_chunks (doc_id, ord, text, embedding_json)
            VALUES (?, ?, ?, ?)
        """, rows)
        conn.commit()
    return len(rows)

def fetch_all_vectors() -> List[Tuple[int, int, int, str, List[float] | np.ndarray, str]]:
    """
    Devuelve: (chunk_id, doc_id, ord, text, emb, titulo)
    emb: lista de floats o np.ndarray (api.py hace np.asarray(...)
    """
    with get_conn() as conn:
        cur = conn.cursor()
        cur.execute("""
            SELECT c.chunk_id, c.doc_id, c.ord, c.text, c.embedding_json, d.titulo
            FROM secop_chunks c
            JOIN secop_documents d ON d.doc_id = c.doc_id
            ORDER BY c.doc_id, c.ord
        """)
        rows = cur.fetchall()
    out: List[Tuple[int, int, int, str, List[float] | np.ndarray, str]] = []
    for r in rows:
        # parse emb como lista de floats; api.py ya convierte a np.float32 para el dot product
        try:
            emb = json.loads(r[4]) if r[4] else []
        except Exception:
            emb = []
        out.append((int(r[0]), int(r[1]), int(r[2]), r[3], emb, r[5]))
    return out

def get_document(doc_id: int) -> Optional[Dict[str, Any]]:
    """Devuelve un dict con al menos: doc_id, titulo, entidad, archivo, metadata."""
    with get_conn() as conn:
        cur = conn.cursor()
        cur.execute("""
            SELECT doc_id, titulo, entidad, source_path, metadata
            FROM secop_documents WHERE doc_id = ?
        """, (doc_id,))
        r = cur.fetchone()
        if not r:
            return None
        row = {
            "doc_id": int(r[0]),
            "titulo": r[1],
            "entidad": r[2],
            "archivo": r[3],  # mapeo a "archivo"
            "metadata": r[4],
        }
        # Normaliza metadata a dict si es JSON
        try:
            row["metadata"] = json.loads(row.get("metadata") or "{}")
        except Exception:
            pass
        return row

def fetch_doc_text(doc_id: int) -> str:
    """Concatena texto de todos los chunks (en orden) de un documento."""
    with get_conn() as conn:
        cur = conn.cursor()
        cur.execute("""
            SELECT text FROM secop_chunks
            WHERE doc_id = ?
            ORDER BY ord ASC
        """, (doc_id,))
        rows = cur.fetchall()
        return "\n".join([r[0] for r in rows]) if rows else ""
